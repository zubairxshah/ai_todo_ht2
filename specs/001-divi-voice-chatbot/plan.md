# Implementation Plan: Divi Voice Chatbot

**Branch**: `001-divi-voice-chatbot` | **Date**: 2025-12-21 | **Spec**: [spec.md](./spec.md)
**Input**: Feature specification from `/specs/001-divi-voice-chatbot/spec.md`

## Summary

Voice-enabled AI chatbot extension named "Divi" that adds speech-to-text capabilities to the existing ChatKitWidget. Users can click a microphone button to speak commands, which are transcribed using the Web Speech API (browser) with OpenAI Whisper API fallback (server-side). The transcribed text is sent through the existing ChatKit/MCP pipeline for processing. Visual feedback provides state awareness during listening, transcription, and response phases.

## Technical Context

**Language/Version**: TypeScript 5.x (frontend), Python 3.11+ (backend)
**Primary Dependencies**:
- Frontend: Web Speech API (browser-native), React hooks for audio handling
- Backend: OpenAI Whisper API (fallback), FastAPI for transcription endpoint
**Storage**: N/A (voice data is ephemeral, not stored)
**Testing**: Jest + React Testing Library (frontend), pytest (backend)
**Target Platform**: Web browsers (Chrome, Firefox, Safari, Edge)
**Project Type**: Web application (extending existing frontend/backend)
**Performance Goals**:
- Transcription latency <2s for typical commands
- Visual feedback within 500ms
- 90%+ accuracy in quiet environments
**Constraints**:
- Must integrate with existing ChatKitWidget
- Must not break existing text input functionality
- Must handle microphone permission gracefully
**Scale/Scope**: Single user at a time, commands <60 seconds

## Constitution Check

*GATE: Must pass before Phase 0 research. Re-check after Phase 1 design.*

- [x] **TDD**: Tests will be written before implementation for voice hooks and components
- [x] **Simplicity/YAGNI**: Using browser-native Web Speech API first, Whisper only as fallback
- [x] **Security-First**:
  - Microphone access requires explicit user permission
  - Audio data not persisted or logged
  - Transcription sent through existing authenticated ChatKit endpoint
- [x] **Type Safety**: TypeScript strict mode, typed interfaces for voice state and events

**Gate Status**: PASSED - No violations

## Project Structure

### Documentation (this feature)

```text
specs/001-divi-voice-chatbot/
├── spec.md              # Feature specification
├── plan.md              # This file
├── research.md          # Speech recognition research
├── data-model.md        # Voice session data structures
├── quickstart.md        # Testing guide
├── contracts/           # API contracts
│   └── transcription-api.md
└── tasks.md             # Implementation tasks (generated by /sp.tasks)
```

### Source Code (repository root)

```text
frontend/
├── src/
│   ├── components/
│   │   ├── Chat/
│   │   │   ├── ChatKitWidget.tsx    # Existing - will extend
│   │   │   ├── VoiceInput.tsx       # NEW: Voice button + recording UI
│   │   │   └── VoiceIndicator.tsx   # NEW: Visual feedback component
│   │   └── ...
│   ├── hooks/
│   │   └── useVoiceInput.ts         # NEW: Voice recording hook
│   ├── lib/
│   │   └── voice-transcription.ts   # NEW: Transcription service
│   └── types/
│       └── voice.ts                 # NEW: Voice-related types

backend/
├── app/
│   ├── routers/
│   │   └── transcription.py         # NEW: Whisper API fallback endpoint
│   └── services/
│       └── whisper.py               # NEW: Whisper API integration
└── tests/
    └── test_transcription.py        # NEW: Transcription tests
```

**Structure Decision**: Extending existing web application structure. Voice components integrate into the Chat/ directory. Backend adds a single fallback endpoint for browsers without Web Speech API support.

## API Contracts

### Frontend Voice Flow

```
User clicks mic → requestMicrophonePermission()
                → startRecording()
                → [visual: "Listening..."]

User speaks     → Web Speech API recognition
                → [visual: "Processing..."]

Speech ends     → onResult(transcription)
                → displayTranscription()
                → sendToChat(transcription)
                → [existing ChatKit flow]

Error           → onError(error)
                → [visual: error message]
                → [fallback: show text input]
```

### Backend Fallback Endpoint

```
POST /api/transcribe
Authorization: Bearer <jwt>
Content-Type: multipart/form-data

Body: audio (file, webm/wav)

Response 200:
{
  "text": "transcribed text",
  "confidence": 0.95,
  "language": "en"
}

Response 400: { "error": "No audio provided" }
Response 401: { "error": "Unauthorized" }
Response 500: { "error": "Transcription failed" }
```

## Key Design Decisions

### Decision 1: Web Speech API First
**Choice**: Use browser's Web Speech API as primary, OpenAI Whisper as fallback
**Rationale**:
- Web Speech API is free, fast, and privacy-friendly (no data sent to servers)
- Whisper provides better accuracy but requires API calls and has cost
- Graceful degradation ensures broad browser support

### Decision 2: Integration Point
**Choice**: Extend ChatKitWidget rather than create separate voice component
**Rationale**:
- Maintains single chat interface with dual input modes
- Reuses existing message display, threading, and task event emission
- Simpler UX - users don't switch between widgets

### Decision 3: Audio Not Persisted
**Choice**: Voice data is ephemeral, only transcription is stored (as chat message)
**Rationale**:
- Privacy-first approach
- Reduces storage requirements
- Transcription text is sufficient for audit/history

## Complexity Tracking

No constitution violations - implementation follows YAGNI principles:
- Single integration point (ChatKitWidget extension)
- Browser-native API preferred over external services
- Minimal new backend code (one fallback endpoint)
